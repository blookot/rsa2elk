# Config file generated by RSA2ELK, see https://github.com/blookot/rsa2elk 
# Author: Vincent Maury
# Check all Netwitness parsers: https://github.com/netwitness/nw-logparsers/tree/master/devices/ (license: Apache 2.0)
# Check this link to search for source configuration guides: https://rsa.jiveon.com/community/products/netwitness/integrations/event-sources

##########
# CAUTION: check the "path" in input and "dictionary_path" in filter, as well as the "template" path in the elasticsearch output or "path" in the file output
##########

input {
#	syslog {
#		port => 514
#	}
#	file {
#		path => "/var/log/example.log"
#		start_position => "beginning"
#		sincedb_path => "/dev/null"
#	}
	generator {
		count => 1
		message => "a log line to test out"
	}
#	kafka {
#		codec => "json"
#		bootstrap_servers => "192.168.30.13:9092"
#		topics => ["mytopic"]
#		security_protocol => "SSL"
#		ssl_key_password => "{ssl_password}"
#		ssl_keystore_location => "/{keystore-absolute-path}"
#		ssl_keystore_password => "{keystore_password}"
#		ssl_truststore_location => "/{truststore-absolute-path}"
#		ssl_truststore_password => "{truststore_password}"
#	}
}

# Renaming a couple of fields
filter {
	mutate {
		rename => {
			"message" => "[event][original]"
			"host" => "[logstash][host]"
		}
	}
}

# Setting the device name and group
filter {
	mutate {
		add_field => {
			"[observer][product]" => "hpnonstopserver"
			"[observer][name]" => "HP Integrity NonStop Server"
			"[observer][type]" => "Analysis"
		}
	}
}


# One single filter block for all headers and messages
filter {

################## HEADERS ##################

	# HEADER 0001
	# line in RSA: <hfld8> %XYGATE Merged Audit@RSA-CATEGORY: <messageid> @0 <!payload:messageid>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<hfld8>[^\s]*)[\s]+%XYGATE[\s]+Merged[\s]+Audit@RSA\-CATEGORY:[\s]+(?<message>(?<messageid>[^\s]*)[\s]+@0[\s]+(?<payload>.*))$" }
			id => "header-0001"
			add_field => {
				"[rsa][header][id]" => "0001"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}



################## MsgId2 to Parser ##################

	translate {
		field => "[rsa][message][id2]"
		destination => "[logstash][msgparser][id]"
		dictionary_path => "msgid2parserid-hpnonstopservermsg.json"
		fallback => ""
		override => true
	}


################## MESSAGES ##################

	# PARSER msgParserId0
	# line in RSA: <fld3>@0 @<fld1>@<fld2>@<sessionid>@<fld4>@<fld5>@<fld6>@<fld7>@<fld8>@<application>@<fld10>@<saddr>@<shost>@<fld13>@<fld14>@<event_source>@<hostip>@<hostname>@<directory>@<fld19>@0 @<fld20>@<fld21>@ <fld22> <fld47>@<fld23>@<fld24>@<sessionid1>@<resultcode>@<fld27>@<fld28>@<severity>@<context>@<fld31>@<fld32>,<fld33>@<fld34>,<fld35>@<username>@<fld37>@<c_username>@<obj_type>@<obj_name>@<action>@<dhost>@<id>@<fld44>@<rulename>@<info>@<event_description>
	if [logstash][msgparser][id] == "msgParserId0" {
		dissect {
			mapping => { "message" => "%{fld3}@0 @%{fld1}@%{fld2}@%{sessionid}@%{fld4}@%{fld5}@%{fld6}@%{fld7}@%{fld8}@%{application}@%{fld10}@%{saddr}@%{shost}@%{fld13}@%{fld14}@%{event_source}@%{hostip}@%{hostname}@%{directory}@%{fld19}@0 @%{fld20}@%{fld21}@ %{fld22} %{fld47}@%{fld23}@%{fld24}@%{sessionid1}@%{resultcode}@%{fld27}@%{fld28}@%{severity}@%{context}@%{fld31}@%{fld32},%{fld33}@%{fld34},%{fld35}@%{username}@%{fld37}@%{c_username}@%{obj_type}@%{obj_name}@%{action}@%{dhost}@%{id}@%{fld44}@%{rulename}@%{info}@%{event_description}" }
			id => "msgParserId0"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld22} %{fld47}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d HH:m:s" ] }
		}
	}
	# PARSER msgParserId1
	# line in RSA: <fld3>@<fld45>@<fld1>@<fld2>@<sessionid>@<fld4>@<fld5>@<fld6>@<fld7>@<fld8>@<application>@<fld10>@<saddr>@<shost>@<fld13>@<fld14>@<event_source>@<hostip>@<hostname>@<directory>@<fld19>@<fld46>@<fld20>@<fld21>@ <fld22> <fld47>@<fld23>@<fld24>@<sessionid1>@<resultcode>@<fld27>@<fld28>@<severity>@<context>@<fld31>@<fld32>,<fld33>@<fld34>,<fld35>@<username>@<fld37>@<c_username>@<obj_type>@<obj_name>@<action>@<dhost>@<id>@<fld44>@<rulename>@<info>@<event_description>
	else if [logstash][msgparser][id] == "msgParserId1" {
		dissect {
			mapping => { "message" => "%{fld3}@%{fld45}@%{fld1}@%{fld2}@%{sessionid}@%{fld4}@%{fld5}@%{fld6}@%{fld7}@%{fld8}@%{application}@%{fld10}@%{saddr}@%{shost}@%{fld13}@%{fld14}@%{event_source}@%{hostip}@%{hostname}@%{directory}@%{fld19}@%{fld46}@%{fld20}@%{fld21}@ %{fld22} %{fld47}@%{fld23}@%{fld24}@%{sessionid1}@%{resultcode}@%{fld27}@%{fld28}@%{severity}@%{context}@%{fld31}@%{fld32},%{fld33}@%{fld34},%{fld35}@%{username}@%{fld37}@%{c_username}@%{obj_type}@%{obj_name}@%{action}@%{dhost}@%{id}@%{fld44}@%{rulename}@%{info}@%{event_description}" }
			id => "msgParserId1"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld22} %{fld47}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d HH:m:s" ] }
		}
	}
	# PARSER msgParserId2
	# line in RSA: <fld3>@0 @<fld1>@<fld2>@<sessionid>@<fld4>@<fld5>@<fld6>@<fld7>@<fld8>@<application>@<fld10>@<saddr>@<shost>@<fld13>@<fld14>@<event_source>@<hostip>@<hostname>@<directory>@<fld19>@0 @<fld20>@<fld21>@ {<fld22>:<fld47>|<fld22> <fld47>}@<fld23>@<fld24>@<sessionid1>@<resultcode>@<fld27>@<fld28>@<severity>@<context>@<fld31>@<fld32>,<fld33>@<fld34>,<fld35>@<username>@<fld37>@<c_username>@<obj_type>@<obj_name>@<action>@<dhost>@<id>@<fld44>@<rulename>@<info>@<event_description>
	else if [logstash][msgparser][id] == "msgParserId2" {
		grok {
			match => { "message" => "^(?<fld3>[^@]*)@0[\s]+@(?<fld1>[^@]*)@(?<fld2>[^@]*)@(?<sessionid>[^@]*)@(?<fld4>[^@]*)@(?<fld5>[^@]*)@(?<fld6>[^@]*)@(?<fld7>[^@]*)@(?<fld8>[^@]*)@(?<application>[^@]*)@(?<fld10>[^@]*)@(?<saddr>[^@]*)@(?<shost>[^@]*)@(?<fld13>[^@]*)@(?<fld14>[^@]*)@(?<event_source>[^@]*)@(?<hostip>[^@]*)@(?<hostname>[^@]*)@(?<directory>[^@]*)@(?<fld19>[^@]*)@0[\s]+@(?<fld20>[^@]*)@(?<fld21>[^@]*)@[\s]+((?<fld22>[^:]*):(?<fld47>[^@]*)|(?<fld22>[^\s]*)[\s]+(?<fld47>[^@]*))@(?<fld23>[^@]*)@(?<fld24>[^@]*)@(?<sessionid1>[^@]*)@(?<resultcode>[^@]*)@(?<fld27>[^@]*)@(?<fld28>[^@]*)@(?<severity>[^@]*)@(?<context>[^@]*)@(?<fld31>[^@]*)@(?<fld32>[^,]*),(?<fld33>[^@]*)@(?<fld34>[^,]*),(?<fld35>[^@]*)@(?<username>[^@]*)@(?<fld37>[^@]*)@(?<c_username>[^@]*)@(?<obj_type>[^@]*)@(?<obj_name>[^@]*)@(?<action>[^@]*)@(?<dhost>[^@]*)@(?<id>[^@]*)@(?<fld44>[^@]*)@(?<rulename>[^@]*)@(?<info>[^@]*)@(?<event_description>.*)$" }
			id => "msgParserId2"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld22} %{fld47}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d:HH:m:s", "yyyy-M-d HH:m:s" ] }
		}
	}
	# PARSER msgParserId3
	# line in RSA: <fld3>@<fld45>@<fld1>@<fld2>@<sessionid>@<fld4>@<fld5>@<fld6>@<fld7>@<fld8>@<application>@<fld10>@<saddr>@<shost>@<fld13>@<fld14>@<event_source>@<hostip>@<hostname>@<directory>@<fld19>@<fld46>@<fld20>@<fld21>@ <fld22>@<fld23>@<fld24>@<sessionid1>@<resultcode>@<fld27>@<fld28>@<severity>@<context>@<fld31>@<fld32>,<fld33>@<fld34>,<fld35>@<username>@<fld37>@<c_username>@<obj_type>@<obj_name>@<action>@<dhost>@<id>@<fld44>@<rulename>@<info>@<event_description>
	else if [logstash][msgparser][id] == "msgParserId3" {
		dissect {
			mapping => { "message" => "%{fld3}@%{fld45}@%{fld1}@%{fld2}@%{sessionid}@%{fld4}@%{fld5}@%{fld6}@%{fld7}@%{fld8}@%{application}@%{fld10}@%{saddr}@%{shost}@%{fld13}@%{fld14}@%{event_source}@%{hostip}@%{hostname}@%{directory}@%{fld19}@%{fld46}@%{fld20}@%{fld21}@ %{fld22}@%{fld23}@%{fld24}@%{sessionid1}@%{resultcode}@%{fld27}@%{fld28}@%{severity}@%{context}@%{fld31}@%{fld32},%{fld33}@%{fld34},%{fld35}@%{username}@%{fld37}@%{c_username}@%{obj_type}@%{obj_name}@%{action}@%{dhost}@%{id}@%{fld44}@%{rulename}@%{info}@%{event_description}" }
			id => "msgParserId3"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld22}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d:HH:m:s", "yyyy-M-d HH:m:s" ] }
		}
	}


################## END OF MESSAGES ##################

# End of the filter block
}


output {
#	if [logstash][headerfound] and [logstash][messagefound] {
#		elasticsearch {
#			hosts => ["https://elasticxxxxxx"]
#			index => "%{[observer][product]}-%{+YYYY.MM.dd}"
#			user => "logstash"
#			password => "abc"	# Better use keystore, cf https://www.elastic.co/guide/en/logstash/master/keystore.html
#			manage_template => true
#			template => "es-mapping-hpnonstopservermsg.json"
#			template_name => "hpnonstopserver_template"
#			template_overwrite => true
#		}
#	} else {
#		# using a file output for logs that were not parsed correctly
#		# should you chose to send it to elasticsearch, please read https://discuss.elastic.co/t/latency-with-2-elasticsearch-systems/170074/2
#		file { path => "failed_logs-%{[observer][product]}-%{+YYYY-MM-dd}" }
#	}
	stdout {
		codec => rubydebug
	}
}
