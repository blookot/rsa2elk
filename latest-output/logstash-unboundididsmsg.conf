# Config file generated by RSA2ELK, see https://github.com/blookot/rsa2elk 
# Author: Vincent Maury
# Check all Netwitness parsers: https://github.com/netwitness/nw-logparsers/tree/master/devices/ (license: Apache 2.0)
# Check this link to search for source configuration guides: https://rsa.jiveon.com/community/products/netwitness/integrations/event-sources

##########
# CAUTION: check the "path" in input and "dictionary_path" in filter, as well as the "template" path in the elasticsearch output or "path" in the file output
##########

input {
#	syslog {
#		port => 514
#	}
#	file {
#		path => "/var/log/example.log"
#		start_position => "beginning"
#		sincedb_path => "/dev/null"
#	}
	generator {
		count => 1
		message => "a log line to test out"
	}
#	kafka {
#		codec => "json"
#		bootstrap_servers => "192.168.30.13:9092"
#		topics => ["mytopic"]
#		security_protocol => "SSL"
#		ssl_key_password => "{ssl_password}"
#		ssl_keystore_location => "/{keystore-absolute-path}"
#		ssl_keystore_password => "{keystore_password}"
#		ssl_truststore_location => "/{truststore-absolute-path}"
#		ssl_truststore_password => "{truststore_password}"
#	}
}

# Renaming a couple of fields
filter {
	mutate {
		rename => {
			"message" => "[event][original]"
			"host" => "[logstash][host]"
		}
	}
}

# Setting the device name and group
filter {
	mutate {
		add_field => {
			"[observer][product]" => "unboundidids"
			"[observer][name]" => "UnboundID Identity Data Store"
			"[observer][type]" => "Access Control"
		}
	}
}


# One single filter block for all headers and messages
filter {

################## HEADERS ##################

	# HEADER 0002
	# line in RSA: <hmonth> <hday> <htime> <hostname> .<messageid>[<hfld1>]: <!payload:hfld1>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<hmonth>[^\s]*)[\s]+(?<hday>[^\s]*)[\s]+(?<htime>[^\s]*)[\s]+(?<hostname>[^\s]*)[\s]+\.(?<messageid>[^\[]*)\[(?<message>(?<hfld1>[^\]]*)\]:[\s]+(?<payload>.*))$" }
			id => "header-0002"
			add_field => {
				"[rsa][header][id]" => "0002"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}
	# HEADER 9999
	# line in RSA: <hmonth> <hday> <hhour>:<hmin>:<hsec> <hostname> <p_msgid1> <p_msgid2> <hfld1> requesterIP=<hfld2> <!payload:hfld1>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<hmonth>[^\s]*)[\s]+(?<hday>[^\s]*)[\s]+(?<hhour>[^:]*):(?<hmin>[^:]*):(?<hsec>[^\s]*)[\s]+(?<hostname>[^\s]*)[\s]+(?<p_msgid1>[^\s]*)[\s]+(?<p_msgid2>[^\s]*)[\s]+(?<message>(?<hfld1>[^\s]*)[\s]+requesterIP=(?<hfld2>[^\s]*)[\s]+(?<payload>.*))$" }
			id => "header-9999"
			add_field => {
				"[rsa][header][id]" => "9999"
				"[rsa][message][id2]" => "UNBOUNDIDIDS_TVM"
				"[logstash][headerfound]" => true
			}
		}
	}



################## MsgId2 to Parser ##################

	translate {
		field => "[rsa][message][id2]"
		destination => "[logstash][msgparser][id]"
		dictionary_path => "msgid2parserid-unboundididsmsg.json"
		fallback => ""
		override => true
	}


################## MESSAGES ##################

	# PARSER msgParserId0
	# line in RSA: <process_id>]: <event_description>
	if [logstash][msgparser][id] == "msgParserId0" {
		dissect {
			mapping => { "message" => "%{process_id}]: %{event_description}" }
			id => "msgParserId0"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{hmonth} %{hday} %{htime}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "MMM d HH:m:s" ] }
		}
	}
	# PARSER msgParserId1
	# line in RSA: additionalInfo=<context> administrativeOperation=<info> attrs=<change_attribute> authDN=<fld> authFailureID=<fld> authFailureReason=<p_result1> authType=<authmethod> authzDN=<dn> base=<dn> clientConnectionPolicy=<policyname> conn=<connectionid> dn=<dn> entriesReturned=<fld> etime=<duration> filter=<fld> from=<context> instanceName=<instance> matchedDN=<fld> message=<event_description> msgID=<id> op=<operation_id> opPurpose=<info> preAuthZUsedPrivileges=<privilege> product=<product> qtime=<fld> replicationChangeID=<fld> requesterDN=<src_dn> requesterIP=<saddr> requestOID=<fld> requestType=<event_type> responseOID=<fld> responseType=<fld> resultCode=<resultcode> resultCodeName=<result> scope=<fld> serversAccessed=<fld> targetHost=<dhost> targetPort=<dport> targetProtocol=<protocol> threadID=<severity> unindexed=<index> usedPrivileges=<fld> usingAdminSessionWorkerThread=<fld> version=<version> via=<context> requestControls=<fld> responseControls=<fld>
	else if [logstash][msgparser][id] == "msgParserId1" {
		dissect {
			mapping => { "message" => "additionalInfo=%{context} administrativeOperation=%{info} attrs=%{change_attribute} authDN=%{fld} authFailureID=%{fld} authFailureReason=%{p_result1} authType=%{authmethod} authzDN=%{dn} base=%{dn} clientConnectionPolicy=%{policyname} conn=%{connectionid} dn=%{dn} entriesReturned=%{fld} etime=%{duration} filter=%{fld} from=%{context} instanceName=%{instance} matchedDN=%{fld} message=%{event_description} msgID=%{id} op=%{operation_id} opPurpose=%{info} preAuthZUsedPrivileges=%{privilege} product=%{product} qtime=%{fld} replicationChangeID=%{fld} requesterDN=%{src_dn} requesterIP=%{saddr} requestOID=%{fld} requestType=%{event_type} responseOID=%{fld} responseType=%{fld} resultCode=%{resultcode} resultCodeName=%{result} scope=%{fld} serversAccessed=%{fld} targetHost=%{dhost} targetPort=%{dport} targetProtocol=%{protocol} threadID=%{severity} unindexed=%{index} usedPrivileges=%{fld} usingAdminSessionWorkerThread=%{fld} version=%{version} via=%{context} requestControls=%{fld} responseControls=%{fld}" }
			id => "msgParserId1"
			add_field => {
				"vid" => "%{p_msgid1}_%{p_msgid2}"
				"ec_theme" => "Communication"
				"p_action" => "%{p_msgid1}\t%{p_msgid2}"
				"result" => "%{p_result1}"
				"[logstash][messagefound]" => true
			}
		}
	}


################## END OF MESSAGES ##################

# End of the filter block
}

# Enrich events using VALUEMAP
filter {
	translate {
		field => "[p_msgid1]"
		destination => "[event_cat]"
		dictionary => {
			"MODIFY" => "1701020000"
		}
		fallback => "1605000000"
		override => true
	}
}
filter {
	translate {
		field => "[event_cat]"
		destination => "[event_cat_name]"
		dictionary => {
			"1701020000" => "Config.Changes.Modify"
		}
		fallback => "System.Normal Conditions"
		override => true
	}
}
filter {
	translate {
		field => "[msg_id]"
		destination => "[ec_activity]"
		dictionary => {
			"SEARCH_REQUEST" => "Request"
			"DELETE_RESULT" => "Delete"
			"DELETE_REQUEST" => "Request"
			"UNBIND_REQUEST" => "Request"
			"BIND_REQUEST" => "Request"
			"MODIFY_REQUEST" => "Request"
			"MODIFY_RESULT" => "Modify"
			"EXTENDED_REQUEST" => "Request"
			"ADD_REQUEST" => "Request"
		}
		fallback => ""
		override => true
	}
}
filter {
	translate {
		field => "[result]"
		destination => "[ec_outcome]"
		dictionary => {
			"Success" => "Success"
			"Failed" => "Failure"
		}
		fallback => ""
		override => true
	}
}


output {
#	if [logstash][headerfound] and [logstash][messagefound] {
#		elasticsearch {
#			hosts => ["https://elasticxxxxxx"]
#			index => "%{[observer][product]}-%{+YYYY.MM.dd}"
#			user => "logstash"
#			password => "abc"	# Better use keystore, cf https://www.elastic.co/guide/en/logstash/master/keystore.html
#			manage_template => true
#			template => "es-mapping-unboundididsmsg.json"
#			template_name => "unboundidids_template"
#			template_overwrite => true
#		}
#	} else {
#		# using a file output for logs that were not parsed correctly
#		# should you chose to send it to elasticsearch, please read https://discuss.elastic.co/t/latency-with-2-elasticsearch-systems/170074/2
#		file { path => "failed_logs-%{[observer][product]}-%{+YYYY-MM-dd}" }
#	}
	stdout {
		codec => rubydebug
	}
}
