# Config file generated by RSA2ELK, see https://github.com/blookot/rsa2elk 
# Author: Vincent Maury
# Check all Netwitness parsers: https://github.com/netwitness/nw-logparsers/tree/master/devices/ (license: Apache 2.0)
# Check this link to search for source configuration guides: https://rsa.jiveon.com/community/products/netwitness/integrations/event-sources

##########
# CAUTION: check the "path" in input and "dictionary_path" in filter, as well as the "template" path in the elasticsearch output or "path" in the file output
##########

input {
#	syslog {
#		port => 514
#	}
#	file {
#		path => "/var/log/example.log"
#		start_position => "beginning"
#		sincedb_path => "/dev/null"
#	}
	generator {
		count => 1
		message => "a log line to test out"
	}
#	kafka {
#		codec => "json"
#		bootstrap_servers => "192.168.30.13:9092"
#		topics => ["mytopic"]
#		security_protocol => "SSL"
#		ssl_key_password => "{ssl_password}"
#		ssl_keystore_location => "/{keystore-absolute-path}"
#		ssl_keystore_password => "{keystore_password}"
#		ssl_truststore_location => "/{truststore-absolute-path}"
#		ssl_truststore_password => "{truststore_password}"
#	}
}

# Renaming a couple of fields
filter {
	mutate {
		rename => {
			"message" => "[event][original]"
			"host" => "[logstash][host]"
		}
	}
}

# Setting the device name and group
filter {
	mutate {
		add_field => {
			"[observer][product]" => "vmware_nsx"
			"[observer][name]" => "VMware NSX"
			"[observer][type]" => "Virtualization"
		}
	}
}


# One single filter block for all headers and messages
filter {

################## HEADERS ##################

	# HEADER 0002
	# line in RSA: <hfld1> <hfld2>T<hfld3>Z <hfld4> <messageid>: <!payload:hfld2>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<hfld1>[^\s]*)[\s]+(?<message>(?<hfld2>[^T]*)T(?<hfld3>[^Z]*)Z[\s]+(?<hfld4>[^\s]*)[\s]+(?<messageid>[^:]*):[\s]+(?<payload>.*))$" }
			id => "header-0002"
			add_field => {
				"[rsa][header][id]" => "0002"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}
	# HEADER 0001
	# line in RSA: <hfld1>T<hfld2>Z <hfld3> <messageid>: <!payload:hfld1>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<message>(?<hfld1>[^T]*)T(?<hfld2>[^Z]*)Z[\s]+(?<hfld3>[^\s]*)[\s]+(?<messageid>[^:]*):[\s]+(?<payload>.*))$" }
			id => "header-0001"
			add_field => {
				"[rsa][header][id]" => "0001"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}
	# HEADER 0003
	# line in RSA: <hfld1> <hfld2> <hfld3> <hhost> <messageid>[<hfld4>]: <!payload:messageid>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<hfld1>[^\s]*)[\s]+(?<hfld2>[^\s]*)[\s]+(?<hfld3>[^\s]*)[\s]+(?<hhost>[^\s]*)[\s]+(?<message>(?<messageid>[^\[]*)\[(?<hfld4>[^\]]*)\]:[\s]+(?<payload>.*))$" }
			id => "header-0003"
			add_field => {
				"[rsa][header][id]" => "0003"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}
	# HEADER 0004
	# line in RSA: <hfld1> <hfld2> <hfld3> <hhost> <messageid>: <!payload:messageid>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^(?<hfld1>[^\s]*)[\s]+(?<hfld2>[^\s]*)[\s]+(?<hfld3>[^\s]*)[\s]+(?<hhost>[^\s]*)[\s]+(?<message>(?<messageid>[^:]*):[\s]+(?<payload>.*))$" }
			id => "header-0004"
			add_field => {
				"[rsa][header][id]" => "0004"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}



################## MsgId2 to Parser ##################

	translate {
		field => "[rsa][message][id2]"
		destination => "[logstash][msgparser][id]"
		dictionary_path => "msgid2parserid-vmware_nsxmsg.json"
		fallback => ""
		override => true
	}


################## MESSAGES ##################

	if [logstash][msgparser][id] == "dfwpktlogs" {
		# MESSAGE dfwpktlogs_L2_Permit
		# line in RSA:        <fld1>T<fld2>Z <hostname> dfwpktlogs: L2 match PASS <fld3>/<rule> <fld4> <fld5> <smacaddr>-><dmacaddr> ETHTYPE <fld6>
		if ![logstash][messagefound] {
			dissect {
				mapping => { "message" => "%{fld1}T%{fld2}Z %{hostname} dfwpktlogs: L2 match PASS %{fld3}/%{rule} %{fld4} %{fld5} %{smacaddr}->%{dmacaddr} ETHTYPE %{fld6}" }
				id => "message-dfwpktlogs_L2_Permit"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
				"ec_activity" => "Permit"
				"result" => "match"
				"disposition" => "PASS"
					"[event][id]" => "dfwpktlogs_L2_Permit"
					"[rsa][message][id1]" => "dfwpktlogs_L2_Permit"
					"[event][categoryid]" => "1801020000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs_match_Permit:01
		# line in RSA:        <fld1>T<fld2>Z <hostname> dfwpktlogs: {INET6|INET} match PASS <fld3>/<rule> <fld4> <fld5> <protocol> <fld8> <saddr>-><daddr>
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^[\s]*(?<fld1>[^T]*)T(?<fld2>[^Z]*)Z[\s]+(?<hostname>[^\s]*)[\s]+dfwpktlogs:[\s]+(INET6|INET)[\s]+match[\s]+PASS[\s]+(?<fld3>[^/]*)/(?<rule>[^\s]*)[\s]+(?<fld4>[^\s]*)[\s]+(?<fld5>[^\s]*)[\s]+(?<protocol>[^\s]*)[\s]+(?<fld8>[^\s]*)[\s]+(?<saddr>[^\-]*)\->(?<daddr>.*)$" }
				id => "message-dfwpktlogs_match_Permit:01"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
				"ec_activity" => "Permit"
				"result" => "match"
				"disposition" => "PASS"
					"[event][id]" => "dfwpktlogs_match_Permit:01"
					"[rsa][message][id1]" => "dfwpktlogs_match_Permit:01"
					"[event][categoryid]" => "1801020000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs_match:01
		# line in RSA:       <fld1>T<fld2>Z <hostname> dfwpktlogs: {INET6|INET} match <disposition> <fld3>/<rule> <fld4> <fld5> <protocol> <fld8> <saddr>-><daddr>
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^[\s]*(?<fld1>[^T]*)T(?<fld2>[^Z]*)Z[\s]+(?<hostname>[^\s]*)[\s]+dfwpktlogs:[\s]+(INET6|INET)[\s]+match[\s]+(?<disposition>[^\s]*)[\s]+(?<fld3>[^/]*)/(?<rule>[^\s]*)[\s]+(?<fld4>[^\s]*)[\s]+(?<fld5>[^\s]*)[\s]+(?<protocol>[^\s]*)[\s]+(?<fld8>[^\s]*)[\s]+(?<saddr>[^\-]*)\->(?<daddr>.*)$" }
				id => "message-dfwpktlogs_match:01"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
				"result" => "match"
					"[event][id]" => "dfwpktlogs_match:01"
					"[rsa][message][id1]" => "dfwpktlogs_match:01"
					"[event][categoryid]" => "1801000000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs_match_Permit:02
		# line in RSA:        <fld1>T<fld2>Z <hostname> dfwpktlogs: {INET6|INET} match PASS <fld3>/<rule> <fld4> <fld5> <protocol> <saddr>/<sport>-><daddr>/{<dport> <fld7>|<dport>}
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^[\s]*(?<fld1>[^T]*)T(?<fld2>[^Z]*)Z[\s]+(?<hostname>[^\s]*)[\s]+dfwpktlogs:[\s]+(INET6|INET)[\s]+match[\s]+PASS[\s]+(?<fld3>[^/]*)/(?<rule>[^\s]*)[\s]+(?<fld4>[^\s]*)[\s]+(?<fld5>[^\s]*)[\s]+(?<protocol>[^\s]*)[\s]+(?<saddr>[^/]*)/(?<sport>[^\-]*)\->(?<daddr>[^/]*)/((?<dport>[^\s]*)[\s]+(?<fld7>.*)|(?<dport>.*))$" }
				id => "message-dfwpktlogs_match_Permit:02"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
				"ec_activity" => "Permit"
				"result" => "match"
				"disposition" => "PASS"
					"[event][id]" => "dfwpktlogs_match_Permit:02"
					"[rsa][message][id1]" => "dfwpktlogs_match_Permit:02"
					"[event][categoryid]" => "1801020000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs_match:02
		# line in RSA:       <fld1>T<fld2>Z <hostname> dfwpktlogs: {INET6|INET} match <disposition> <fld3>/<rule> <fld4> <fld5> <protocol> <saddr>/<sport>-><daddr>/{<dport> <fld7>|<dport>}
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^[\s]*(?<fld1>[^T]*)T(?<fld2>[^Z]*)Z[\s]+(?<hostname>[^\s]*)[\s]+dfwpktlogs:[\s]+(INET6|INET)[\s]+match[\s]+(?<disposition>[^\s]*)[\s]+(?<fld3>[^/]*)/(?<rule>[^\s]*)[\s]+(?<fld4>[^\s]*)[\s]+(?<fld5>[^\s]*)[\s]+(?<protocol>[^\s]*)[\s]+(?<saddr>[^/]*)/(?<sport>[^\-]*)\->(?<daddr>[^/]*)/((?<dport>[^\s]*)[\s]+(?<fld7>.*)|(?<dport>.*))$" }
				id => "message-dfwpktlogs_match:02"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
				"result" => "match"
					"[event][id]" => "dfwpktlogs_match:02"
					"[rsa][message][id1]" => "dfwpktlogs_match:02"
					"[event][categoryid]" => "1801000000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs_TERM:01
		# line in RSA:     <fld1>T<fld2>Z <hostname> dfwpktlogs: {INET6|INET} TERM <fld3>/<rule> <fld4> <protocol> <fld8> <saddr>-><daddr> <fld9> <fld10>
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^[\s]*(?<fld1>[^T]*)T(?<fld2>[^Z]*)Z[\s]+(?<hostname>[^\s]*)[\s]+dfwpktlogs:[\s]+(INET6|INET)[\s]+TERM[\s]+(?<fld3>[^/]*)/(?<rule>[^\s]*)[\s]+(?<fld4>[^\s]*)[\s]+(?<protocol>[^\s]*)[\s]+(?<fld8>[^\s]*)[\s]+(?<saddr>[^\-]*)\->(?<daddr>[^\s]*)[\s]+(?<fld9>[^\s]*)[\s]+(?<fld10>.*)$" }
				id => "message-dfwpktlogs_TERM:01"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
					"[event][id]" => "dfwpktlogs_TERM:01"
					"[rsa][message][id1]" => "dfwpktlogs_TERM:01"
					"[event][categoryid]" => "1801000000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs_TERM:02
		# line in RSA:     <fld1>T<fld2>Z <hostname> dfwpktlogs: {INET6|INET} TERM <fld3>/<rule> <fld4> <protocol> <saddr>/<sport>-><daddr>/<dport> <fld9> <fld10>
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^[\s]*(?<fld1>[^T]*)T(?<fld2>[^Z]*)Z[\s]+(?<hostname>[^\s]*)[\s]+dfwpktlogs:[\s]+(INET6|INET)[\s]+TERM[\s]+(?<fld3>[^/]*)/(?<rule>[^\s]*)[\s]+(?<fld4>[^\s]*)[\s]+(?<protocol>[^\s]*)[\s]+(?<saddr>[^/]*)/(?<sport>[^\-]*)\->(?<daddr>[^/]*)/(?<dport>[^\s]*)[\s]+(?<fld9>[^\s]*)[\s]+(?<fld10>.*)$" }
				id => "message-dfwpktlogs_TERM:02"
				add_field => {
				"ec_subject" => "NetworkComm"
				"ec_theme" => "Communication"
					"[event][id]" => "dfwpktlogs_TERM:02"
					"[rsa][message][id1]" => "dfwpktlogs_TERM:02"
					"[event][categoryid]" => "1801000000"
					"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "yyyy-M-d H:m:s" ] }
			}
		}
		# MESSAGE dfwpktlogs:01
		# line in RSA:     <fld1>T<fld2>Z<hostname>dfwpktlogs:<fld3>{INET6|INET} match<disposition><fld4>/<rule><fld5><fld6><protocol>{<saddr>/<sport>-><daddr>/<dport><fld7>|<saddr>/<sport>-><daddr>/<dport>|<fld11><saddr>-><daddr>|<saddr>-><daddr>}
		# Parsing error: Couldn't parse because of 2 adjacent fields like <fld1><fld2>
		# MESSAGE dfwpktlogs:02
		# line in RSA:    <fld1>T<fld2>Z<hostname>dfwpktlogs:<fld3>{INET6|INET} TERM<fld4>/<rule><fld5><protocol>{<fld11><saddr>/<sport>-><daddr>/<dport>|<saddr>/<sport>-><daddr>/<dport>|<fld11><saddr>-><daddr>|<saddr>-><daddr>}<fld9><fld10>
		# Parsing error: Couldn't parse because of 2 adjacent fields like <fld1><fld2>
	}
	# PARSER msgParserId9
	# line in RSA: <agent>[<process_id>]: [<fld1>]: <fld2>IN=<sinterface> OUT=<dinterface> SRC=<saddr> DST=<daddr> LEN=<fld3> TOS=<fld4> PREC=<fld5> TTL=<duration> ID=<fld6>  {DF PROTO=<protocol> SPT=<sport> DPT=<dport> WINDOW=<fld8> RES=<resultcode> <fld9> URGP=<fld10>|PROTO=<protocol> SPT=<sport> DPT=<dport> LEN=<fld11>|DF PROTO=<protocol>}
	else if [logstash][msgparser][id] == "msgParserId9" {
		grok {
			match => { "message" => "^(?<agent>[^\[]*)\[(?<process_id>[^\]]*)\]:[\s]+\[(?<fld1>[^\]]*)\]:[\s]+(?<fld2>[^I]*)IN=(?<sinterface>[^\s]*)[\s]+OUT=(?<dinterface>[^\s]*)[\s]+SRC=(?<saddr>[^\s]*)[\s]+DST=(?<daddr>[^\s]*)[\s]+LEN=(?<fld3>[^\s]*)[\s]+TOS=(?<fld4>[^\s]*)[\s]+PREC=(?<fld5>[^\s]*)[\s]+TTL=(?<duration>[^\s]*)[\s]+ID=(?<fld6>[^\s]*)[\s]+(DF[\s]+PROTO=(?<protocol>[^\s]*)[\s]+SPT=(?<sport>[^\s]*)[\s]+DPT=(?<dport>[^\s]*)[\s]+WINDOW=(?<fld8>[^\s]*)[\s]+RES=(?<resultcode>[^\s]*)[\s]+(?<fld9>[^\s]*)[\s]+URGP=(?<fld10>.*)|PROTO=(?<protocol>[^\s]*)[\s]+SPT=(?<sport>[^\s]*)[\s]+DPT=(?<dport>[^\s]*)[\s]+LEN=(?<fld11>.*)|DF[\s]+PROTO=(?<protocol>.*))$" }
			id => "msgParserId9"
			add_field => {
				"[logstash][messagefound]" => true
			}
		}
	}
	else if [logstash][msgparser][id] == "nat" {
		# MESSAGE nat:01
		# line in RSA: <agent>[<process_id>]: [<fld1>]: <fld2> SRC=<saddr> DST=<daddr> LEN=<fld3> TOS=<fld4> PREC=<fld5> TTL=<duration> ID=<fld6> DF PROTO=<protocol> SPT=<sport> DPT=<dport> LEN=<fld8>
		if ![logstash][messagefound] {
			dissect {
				mapping => { "message" => "%{agent}[%{process_id}]: [%{fld1}]: %{fld2} SRC=%{saddr} DST=%{daddr} LEN=%{fld3} TOS=%{fld4} PREC=%{fld5} TTL=%{duration} ID=%{fld6} DF PROTO=%{protocol} SPT=%{sport} DPT=%{dport} LEN=%{fld8}" }
				id => "message-nat:01"
				add_field => {
					"[event][id]" => "nat:01"
					"[rsa][message][id1]" => "nat:01"
					"[event][categoryid]" => "1801000000"
					"[logstash][messagefound]" => true
				}
			}
		}
		# MESSAGE nat:02
		# line in RSA: <agent>[<process_id>]: [<fld1>]: <fld2>IN=<sinterface> OUT=<dinterface> {MAC=<macaddr> SRC=<saddr>|SRC=<saddr>} DST=<daddr> LEN=<fld3> TOS=<fld4> PREC=<fld5> TTL=<duration> ID=<fld6> DF PROTO=<protocol> SPT=<sport> DPT=<dport> WINDOW=<fld8> RES=<resultcode> <fld9> URGP=<fld10>
		if ![logstash][messagefound] {
			grok {
				match => { "message" => "^(?<agent>[^\[]*)\[(?<process_id>[^\]]*)\]:[\s]+\[(?<fld1>[^\]]*)\]:[\s]+(?<fld2>[^I]*)IN=(?<sinterface>[^\s]*)[\s]+OUT=(?<dinterface>[^\s]*)[\s]+(MAC=(?<macaddr>[^\s]*)[\s]+SRC=(?<saddr>[^\s]*)|SRC=(?<saddr>[^\s]*))[\s]+DST=(?<daddr>[^\s]*)[\s]+LEN=(?<fld3>[^\s]*)[\s]+TOS=(?<fld4>[^\s]*)[\s]+PREC=(?<fld5>[^\s]*)[\s]+TTL=(?<duration>[^\s]*)[\s]+ID=(?<fld6>[^\s]*)[\s]+DF[\s]+PROTO=(?<protocol>[^\s]*)[\s]+SPT=(?<sport>[^\s]*)[\s]+DPT=(?<dport>[^\s]*)[\s]+WINDOW=(?<fld8>[^\s]*)[\s]+RES=(?<resultcode>[^\s]*)[\s]+(?<fld9>[^\s]*)[\s]+URGP=(?<fld10>.*)$" }
				id => "message-nat:02"
				add_field => {
				"hostname" => "%{hhost}"
					"[event][id]" => "nat:02"
					"[rsa][message][id1]" => "nat:02"
					"[event][categoryid]" => "1801000000"
					"[logstash][fullDateTimeString]" => "%{hfld1} %{hfld2} %{hfld3}"
					"[logstash][messagefound]" => true
				}
			}
			if [logstash][fullDateTimeString] {
				date { match => [ "[logstash][fullDateTimeString]", "MMM d HH:m:s" ] }
			}
		}
	}
	# PARSER msgParserId12
	# line in RSA: <agent>[<process_id>]: [<fld1>]: <hostip>:<network_port> [<fld2>]<info>
	else if [logstash][msgparser][id] == "msgParserId12" {
		dissect {
			mapping => { "message" => "%{agent}[%{process_id}]: [%{fld1}]: %{hostip}:%{network_port} [%{fld2}]%{info}" }
			id => "msgParserId12"
			add_field => {
				"hostname" => "%{hhost}"
				"[logstash][fullDateTimeString]" => "%{hfld1} %{hfld2} %{hfld3}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "MMM d HH:m:s" ] }
		}
	}
	# PARSER msgParserId13
	# line in RSA: <agent>: <severity> :: <fld1>
	else if [logstash][msgparser][id] == "msgParserId13" {
		dissect {
			mapping => { "message" => "%{agent}: %{severity} :: %{fld1}" }
			id => "msgParserId13"
			add_field => {
				"[logstash][messagefound]" => true
			}
		}
	}


################## END OF MESSAGES ##################

# End of the filter block
}

# Enrich events using VALUEMAP
filter {
	translate {
		field => "[disposition]"
		destination => "[ec_activity]"
		dictionary => {
			"REJECT" => "Deny"
			"DROP" => "Deny"
		}
		fallback => ""
		override => true
	}
}
filter {
	translate {
		field => "[fld5]"
		destination => "[direction]"
		dictionary => {
			"IN" => "Inbound"
			"OUT" => "Outbound"
		}
		fallback => ""
		override => true
	}
}


output {
#	if [logstash][headerfound] and [logstash][messagefound] {
#		elasticsearch {
#			hosts => ["https://elasticxxxxxx"]
#			index => "%{[observer][product]}-%{+YYYY.MM.dd}"
#			user => "logstash"
#			password => "abc"	# Better use keystore, cf https://www.elastic.co/guide/en/logstash/master/keystore.html
#			manage_template => true
#			template => "es-mapping-vmware_nsxmsg.json"
#			template_name => "vmware_nsx_template"
#			template_overwrite => true
#		}
#	} else {
#		# using a file output for logs that were not parsed correctly
#		# should you chose to send it to elasticsearch, please read https://discuss.elastic.co/t/latency-with-2-elasticsearch-systems/170074/2
#		file { path => "failed_logs-%{[observer][product]}-%{+YYYY-MM-dd}" }
#	}
	stdout {
		codec => rubydebug
	}
}
