# Config file generated by RSA2ELK, see https://github.com/blookot/rsa2elk 
# Author: Vincent Maury
# Check all Netwitness parsers: https://github.com/netwitness/nw-logparsers/tree/master/devices/ (license: Apache 2.0)
# Check this link to search for source configuration guides: https://rsa.jiveon.com/community/products/netwitness/integrations/event-sources

##########
# CAUTION: check the "path" in input and "dictionary_path" in filter, as well as the "template" path in the elasticsearch output or "path" in the file output
##########

input {
#	syslog {
#		port => 514
#	}
#	file {
#		path => "/var/log/example.log"
#		start_position => "beginning"
#		sincedb_path => "/dev/null"
#	}
	generator {
		count => 1
		message => "a log line to test out"
	}
#	kafka {
#		codec => "json"
#		bootstrap_servers => "192.168.30.13:9092"
#		topics => ["mytopic"]
#		security_protocol => "SSL"
#		ssl_key_password => "{ssl_password}"
#		ssl_keystore_location => "/{keystore-absolute-path}"
#		ssl_keystore_password => "{keystore_password}"
#		ssl_truststore_location => "/{truststore-absolute-path}"
#		ssl_truststore_password => "{truststore_password}"
#	}
}

# Renaming a couple of fields
filter {
	mutate {
		rename => {
			"message" => "[event][original]"
			"host" => "[logstash][host]"
		}
	}
}

# Setting the device name and group
filter {
	mutate {
		add_field => {
			"[observer][product]" => "ibmims"
			"[observer][name]" => "IBM Mainframe (IMS)"
			"[observer][type]" => "Mainframe"
		}
	}
}


# One single filter block for all headers and messages
filter {

################## HEADERS ##################

	# HEADER 0001
	# line in RSA: %IBMIMS-<level>-<messageid>: <id>|<date>|<time>|<!payload:id>
	if ![logstash][headerfound] {
		grok {
			match => { "[event][original]" => "^%IBMIMS\-(?<level>[^\-]*)\-(?<messageid>[^:]*):[\s]+(?<message>(?<id>[^\|]*)\|(?<date>[^\|]*)\|(?<time>[^\|]*)\|(?<payload>.*))$" }
			id => "header-0001"
			add_field => {
				"[rsa][header][id]" => "0001"
				"[rsa][message][id2]" => "%{messageid}"
				"[logstash][headerfound]" => true
			}
		}
	}
	# HEADER 0002
	# line in RSA: %IBMIMSTVM-4: <!payload>
	if ![logstash][headerfound] {
		dissect {
			mapping => { "[event][original]" => "%IBMIMSTVM-4: %{message}" }
			id => "header-0002"
			add_field => {
				"[rsa][header][id]" => "0002"
				"[rsa][message][id2]" => "IBMIMS_TVM"
				"[logstash][headerfound]" => true
			}
		}
	}



################## MsgId2 to Parser ##################

	translate {
		field => "[rsa][message][id2]"
		destination => "[logstash][msgparser][id]"
		dictionary_path => "msgid2parserid-v20_ibmimsmsg.json"
		fallback => ""
		override => true
	}


################## MESSAGES ##################

	# PARSER msgParserId0
	# line in RSA: <id>|<fld1>|<fld2>|COMMAND|<username>|<action>|
	if [logstash][msgparser][id] == "msgParserId0" {
		dissect {
			mapping => { "message" => "%{id}|%{fld1}|%{fld2}|COMMAND|%{username}|%{action}|" }
			id => "msgParserId0"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy.D HH:mm:ss" ] }
		}
	}
	# PARSER msgParserId1
	# line in RSA: <id>|<fld1>|<fld2>|<action>|<username>|<terminal>|<fld3>|
	else if [logstash][msgparser][id] == "msgParserId1" {
		dissect {
			mapping => { "message" => "%{id}|%{fld1}|%{fld2}|%{action}|%{username}|%{terminal}|%{fld3}|" }
			id => "msgParserId1"
			add_field => {
				"ec_subject" => "User"
				"ec_activity" => "Logon"
				"ec_theme" => "Authentication"
				"ec_outcome" => "Success"
				"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy.D HH:mm:ss" ] }
		}
	}
	# PARSER msgParserId2
	# line in RSA: <id>|<fld1>|<fld2>|<fld3>|<username>|<fld4>|
	else if [logstash][msgparser][id] == "msgParserId2" {
		dissect {
			mapping => { "message" => "%{id}|%{fld1}|%{fld2}|%{fld3}|%{username}|%{fld4}|" }
			id => "msgParserId2"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy.D HH:mm:ss" ] }
		}
	}
	# PARSER msgParserId3
	# line in RSA: <id>|<fld1>|<fld2>|<username>|
	else if [logstash][msgparser][id] == "msgParserId3" {
		dissect {
			mapping => { "message" => "%{id}|%{fld1}|%{fld2}|%{username}|" }
			id => "msgParserId3"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "yyyy.D HH:mm:ss" ] }
		}
	}
	# PARSER msgParserId4
	# line in RSA: RecordType=<event_id>|RecordTypeDesc=<event_type>|Date=<fld1>|Time=<fld2>|CNTName=<username>|Command=<resource>|Sysid=<hostid>|Reason=<fld13>|PSB=<fld18>|JobName=<fld19>|StepName=<fld20>|UserId=<user_id>|DB_GU_Calls=<fld22>|DB_GN_Calls=<fld23>|DB_GNP_Calls=<fld24>|DB_GHU_Calls=<fld25>|DB_GHN_Calls=<fld26>|DB_GHNP_Calls=<fld27>|DB_ISRT_Calls=<fld28>|DB_DLET_Calls=<fld29>|DB_REPL_Calls=<fld30>|DB_DL/I_Calls=<fld31>|Msg_Queue_GU_Calls=<fld32>|Msg_Queue_GN_Calls=<fld33>|Msg_Queue_ISRT_Calls=<fld34>|Msg_Queue_Purge_Calls=<fld35>|Test_Enqueues=<fld36>|Test_Enqueue_Waits=<fld37>|Test_Dequeues=<fld38>|Queue_Cmd_Enqueues=<fld39>|Waits_on_Queue_Cmds=<fld40>|Queue_Cmd_Dequeues=<fld41>|Upd_Enqueues=<fld42>|Upd_Enqueue_Waits=<fld43>|Upd_Dequeues=<fld44>|Excl_Encueues=<fld45>|Excl_Enqueue_Waits=<fld46>|Excl_Dequeues=<fld47>|Cmd_Calls=<fld48>|GCmd_Calls=<fld49>|DL/I_Msg_Chg_Calls=<fld50>|DL/I_Msg_Auth_Calls=<fld51>|DL/I_Msg_SETO_Calls=<fld52>|DL/I_APSB_Calls=<fld53>|DL/I_DPSB_Calls=<fld54>|DL/I_GMSG_Calls=<fld55>|DL/I_ICMD_Calls=<fld56>|DL/I_RCMD_Calls=<fld57>|DL/I_CHKP_Calls=<fld58>|DL/I_XRST_Calls=<fld59>|DL/I_ROLB_Calls=<fld60>|DL/I_ROLS_Calls=<fld61>|DL/I_SETS_Calls=<fld62>|DL/I_SETU_Calls=<fld63>|DL/I_INIT_Calls=<fld64>|DL/I_INQY_Calls=<fld65>|DL/I_LOG_Calls=<fld66>|DL/I_DB_Dequeue_Calls=<fld67>|SignFunction=<action>|SignQualifier=<fld73>|UnchangedUserID=<username>|Terminal=<terminal>|SAFGroupName=<facilityname>|SubsysName=<program>|SubsysID=<misc>|Subtype=<id>|SPQBName=<username>|
	else if [logstash][msgparser][id] == "msgParserId4" {
		dissect {
			mapping => { "message" => "RecordType=%{event_id}|RecordTypeDesc=%{event_type}|Date=%{fld1}|Time=%{fld2}|CNTName=%{username}|Command=%{resource}|Sysid=%{hostid}|Reason=%{fld13}|PSB=%{fld18}|JobName=%{fld19}|StepName=%{fld20}|UserId=%{user_id}|DB_GU_Calls=%{fld22}|DB_GN_Calls=%{fld23}|DB_GNP_Calls=%{fld24}|DB_GHU_Calls=%{fld25}|DB_GHN_Calls=%{fld26}|DB_GHNP_Calls=%{fld27}|DB_ISRT_Calls=%{fld28}|DB_DLET_Calls=%{fld29}|DB_REPL_Calls=%{fld30}|DB_DL/I_Calls=%{fld31}|Msg_Queue_GU_Calls=%{fld32}|Msg_Queue_GN_Calls=%{fld33}|Msg_Queue_ISRT_Calls=%{fld34}|Msg_Queue_Purge_Calls=%{fld35}|Test_Enqueues=%{fld36}|Test_Enqueue_Waits=%{fld37}|Test_Dequeues=%{fld38}|Queue_Cmd_Enqueues=%{fld39}|Waits_on_Queue_Cmds=%{fld40}|Queue_Cmd_Dequeues=%{fld41}|Upd_Enqueues=%{fld42}|Upd_Enqueue_Waits=%{fld43}|Upd_Dequeues=%{fld44}|Excl_Encueues=%{fld45}|Excl_Enqueue_Waits=%{fld46}|Excl_Dequeues=%{fld47}|Cmd_Calls=%{fld48}|GCmd_Calls=%{fld49}|DL/I_Msg_Chg_Calls=%{fld50}|DL/I_Msg_Auth_Calls=%{fld51}|DL/I_Msg_SETO_Calls=%{fld52}|DL/I_APSB_Calls=%{fld53}|DL/I_DPSB_Calls=%{fld54}|DL/I_GMSG_Calls=%{fld55}|DL/I_ICMD_Calls=%{fld56}|DL/I_RCMD_Calls=%{fld57}|DL/I_CHKP_Calls=%{fld58}|DL/I_XRST_Calls=%{fld59}|DL/I_ROLB_Calls=%{fld60}|DL/I_ROLS_Calls=%{fld61}|DL/I_SETS_Calls=%{fld62}|DL/I_SETU_Calls=%{fld63}|DL/I_INIT_Calls=%{fld64}|DL/I_INQY_Calls=%{fld65}|DL/I_LOG_Calls=%{fld66}|DL/I_DB_Dequeue_Calls=%{fld67}|SignFunction=%{action}|SignQualifier=%{fld73}|UnchangedUserID=%{username}|Terminal=%{terminal}|SAFGroupName=%{facilityname}|SubsysName=%{program}|SubsysID=%{misc}|Subtype=%{id}|SPQBName=%{username}|" }
			id => "msgParserId4"
			add_field => {
				"[logstash][fullDateTimeString]" => "%{fld1} %{fld2}"
				"[logstash][messagefound]" => true
			}
		}
		if [logstash][fullDateTimeString] {
			date { match => [ "[logstash][fullDateTimeString]", "M-d-yyyy HH:mm:ss" ] }
		}
	}


################## END OF MESSAGES ##################

# End of the filter block
}

# Enrich events using VALUEMAP
filter {
	translate {
		field => "[id]"
		destination => "[event_type]"
		dictionary => {
			"03" => "Message_Queue"
			"16" => "Logon_Record"
			"56" => "Support_IMS_program_synch_points"
			"72" => "ETO_Structure_Deleted_Signoff"
		}
		fallback => ""
		override => true
	}
}


output {
#	if [logstash][headerfound] and [logstash][messagefound] {
#		elasticsearch {
#			hosts => ["https://elasticxxxxxx"]
#			index => "%{[observer][product]}-%{+YYYY.MM.dd}"
#			user => "logstash"
#			password => "abc"	# Better use keystore, cf https://www.elastic.co/guide/en/logstash/master/keystore.html
#			manage_template => true
#			template => "es-mapping-v20_ibmimsmsg.json"
#			template_name => "ibmims_template"
#			template_overwrite => true
#		}
#	} else {
#		# using a file output for logs that were not parsed correctly
#		# should you chose to send it to elasticsearch, please read https://discuss.elastic.co/t/latency-with-2-elasticsearch-systems/170074/2
#		file { path => "failed_logs-%{[observer][product]}-%{+YYYY-MM-dd}" }
#	}
	stdout {
		codec => rubydebug
	}
}
